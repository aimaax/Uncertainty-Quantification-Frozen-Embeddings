import os

from os.path import join as ospj
from os.path import expanduser
from munch import Munch as mch
from tqdm import tqdm_notebook
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import utils

import clip
import ds
from ds import prepare_coco_dataloaders
from tqdm import tqdm
from losses import *
from utils import *

from ds.vocab import Vocabulary

from networks import *




def eval_ProbVLM(
    CLIP_Net,
    BayesCap_Net,
    eval_loader,
    device='cuda',
    dtype=torch.cuda.FloatTensor,
):
    CLIP_Net.to(device)
    CLIP_Net.eval()
    BayesCap_Net.to(device)
    BayesCap_Net.eval()

    mean_mse = 0
    mean_mae = 0
    num_imgs = 0
    list_error = []
    list_var = []
    with tqdm(eval_loader, unit='batch') as tepoch:
        for (idx, batch) in enumerate(tepoch):
            tepoch.set_description('Validating ...')
            ##
            xI, xT  = batch[0].to(device), batch[1].to(device)
            # xI, xT = xI.type(dtype), xT.type(dtype)
            
            # pass them through the network
            with torch.no_grad():
                xfI, xfT = CLIP_Net(xI, xT)
                (img_mu, img_1alpha, img_beta), (txt_mu, txt_1alpha, txt_beta) = BayesCap_Net(xfI, xfT)
                
            n_batch = img_mu.shape[0]
            for j in range(n_batch):
                num_imgs += 1
                mean_mse += emb_mse(img_mu[j], xfI[j]) + emb_mse(txt_mu[j], xfT[j])
                mean_mae += emb_mae(img_mu[j], xfI[j]) + emb_mae(txt_mu[j], xfT[j])
            ##
        mean_mse /= num_imgs
        mean_mae /= num_imgs
        print(
            'Avg. MSE: {} | Avg. MAE: {}'.format
            (
                mean_mse, mean_mae 
            )
        )
    return mean_mae

def load_and_evaluate(
    ckpt_path='../ckpt/ProbVLM_Net_best.pth',
    dataset="coco",
    data_dir="../datasets/coco",
    batch_size=64,
    device='cuda'
    ):
    # Load data loaders
    dataloader_config = mch({
        "batch_size": batch_size,
        "random_erasing_prob": 0,
        "traindata_shuffle": True
    })

    loaders = load_data_loader(dataset, data_dir, dataloader_config)
    coco_valid_loader = loaders['val']

    # Load CLIP model
    CLIP_Net = load_model(device=device, model_path=None)

    # Define BayesCap network
    ProbVLM_Net = BayesCap_for_CLIP(
        inp_dim=512,
        out_dim=512,
        hid_dim=256,
        num_layers=3
    )

    # Load the checkpoint
    print(f"Loading checkpoint from {ckpt_path}")
    ProbVLM_Net.load_state_dict(torch.load(ckpt_path, map_location=device))

    # Evaluate using the existing `eval_ProbVLM` function
    mean_mae = eval_ProbVLM(
        CLIP_Net,
        ProbVLM_Net,
        coco_valid_loader,
        device=device
    )

    # Print or calculate additional statistics if needed
    print(f"Mean MAE from evaluation: {mean_mae}")

    # Return the evaluated metrics
    return mean_mae

def eval_ProbVLM_uncert(
    CLIP_Net,
    BayesCap_Net,
    eval_loader,
    device='cuda',
    n_fw=15,  # Number of forward passes for uncertainty estimation
    bins_type='eq_samples',  # 'eq_spacing' or 'eq_samples'
    n_bins=5,  # Number of uncertainty bins
):
    CLIP_Net.to(device)
    CLIP_Net.eval()
    BayesCap_Net.to(device)
    BayesCap_Net.eval()

    # Get features and uncertainties
    r_dict = get_features_uncer_ProbVLM(CLIP_Net, BayesCap_Net, eval_loader)

    # Sort samples by uncertainty
    sort_v_idx, sort_t_idx = sort_wrt_uncer(r_dict)

    # Bin the sorted samples
    if bins_type == 'eq_spacing':
        bins = create_uncer_bins_eq_spacing(sort_v_idx, n_bins=n_bins)
    elif bins_type == 'eq_samples':
        bins = create_uncer_bins_eq_samples(sort_v_idx, n_bins=n_bins)
    else:
        raise ValueError("Invalid `bins_type`. Choose 'eq_spacing' or 'eq_samples'.")

    # Calculate recall@1 for each bin
    bin_recalls = []
    counter = 0
    for bin_key, samples in bins.items():
        if not samples:
            bin_recalls.append(0)  # If bin is empty, append 0
            continue
        
        indices = [sample[0] for sample in samples]  # Extract indices from sorted list
        bin_query_features = torch.stack([r_dict['i_f'][i] for i in indices])
        bin_gallery_features = torch.stack(r_dict['t_f'][i] for i in indices)  # All gallery features
        pred_ranks = get_pred_ranks(bin_query_features, bin_gallery_features, recall_ks=(1,))
        #if counter == 0:
            #print(f"Query: {bin_query_features}")
            #print(f"Gallery: {bin_gallery_features}")
            #print(f"Pred Ranks: {pred_ranks}")
            #print(f"Indices: {indices}")
        counter += 1
        recall_scores = get_recall_COCOFLICKR(pred_ranks, recall_ks=(1,), q_idx=indices)
        bin_recalls.append(recall_scores[0])

    return bin_recalls, bins


def load_and_evaluate_uncert(
    ckpt_path='../ckpt/ProbVLM_Net_best.pth',
    dataset="coco",
    data_dir="../datasets/coco",
    batch_size=64,
    device='cuda',
    n_bins=5,
    bins_type='eq_samples',
):
    # Load data loaders
    dataloader_config = mch({
        "batch_size": batch_size,
        "random_erasing_prob": 0,
        "traindata_shuffle": True
    })

    loaders = load_data_loader(dataset, data_dir, dataloader_config)
    coco_valid_loader = loaders['val']

    # Load CLIP model
    CLIP_Net = load_model(device=device, model_path=None)

    # Define BayesCap network
    ProbVLM_Net = BayesCap_for_CLIP(
        inp_dim=512,
        out_dim=512,
        hid_dim=256,
        num_layers=3
    )

    # Load the checkpoint
    print(f"Loading checkpoint from {ckpt_path}")
    ProbVLM_Net.load_state_dict(torch.load(ckpt_path, map_location=device))

    # Evaluate with uncertainty
    bin_recalls, bins = eval_ProbVLM_uncert(
        CLIP_Net,
        ProbVLM_Net,
        coco_valid_loader,
        device=device,
        n_bins=n_bins,
        bins_type=bins_type
    )

    # Plot recall@1 vs uncertainty bins
    bin_labels = [f'Bin {i+1}' for i in range(len(bin_recalls))]
    plt.figure(figsize=(10, 6))
    plt.plot(bin_labels, bin_recalls, color='skyblue')
    plt.xlabel('Uncertainty Bins')
    plt.ylabel('Recall@1')
    plt.title('Recall@1 vs. Binned Uncertainty Levels')
    plt.xticks(rotation=45)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig("recall_plot.png")

    return bin_recalls, bins

def compare_iod_vs_ood(
    ckpt_path="../ckpt/ProbVLM_Net_best.pth",
    iod_dataset="coco",
    ood_dataset="flickr",
    data_dir_iod="../datasets/coco",
    data_dir_ood="../datasets/flickr",
    batch_size=16,
    n_fw=10,
    device="cuda",
):
    # Load data loaders
    dataloader_config = mch({
        "batch_size": batch_size,
        "random_erasing_prob": 0,
        "traindata_shuffle": True
    })

    loaders_iod = load_data_loader(iod_dataset, data_dir_iod, dataloader_config)
    loaders_ood = load_data_loader(ood_dataset, data_dir_ood, dataloader_config)
    iod_valid_loader = loaders_iod['val']
    ood_valid_loader = loaders_ood['val']

    # Load CLIP model
    CLIP_Net = load_model(device=device, model_path=None)

    # Define BayesCap network
    Net = BayesCap_for_CLIP(
        inp_dim=512,
        out_dim=512,
        hid_dim=256,
        num_layers=3
    )

    # Load the checkpoint
    print(f"Loading checkpoint from {ckpt_path}")
    Net.load_state_dict(torch.load(ckpt_path, map_location=device))

    # Helper function to estimate `i_v` and `t_v` from a dataloader
    def estimate_uncertainty_variances(loader, description, BayesCap_Net, CLIP_Net, n_fw=15, device="cuda"):
        """
        Estimate uncertainty variances for image and text inputs.

        Args:
            loader: DataLoader providing batches of (image, text) data.
            description: Description for logging (e.g., 'IOD' or 'OOD').
            BayesCap_Net: Bayesian network for uncertainty estimation.
            CLIP_Net: Pretrained CLIP model to extract features.
            n_fw: Number of forward passes to perform.
        
        Returns:
            avg_image_uncertainty: Average uncertainty for image inputs.
            avg_text_uncertainty: Average uncertainty for text inputs.
        """
        image_uncertainties = []
        text_uncertainties = []

        CLIP_Net.to(device)
        CLIP_Net.eval()
        BayesCap_Net.to(device)
        BayesCap_Net.eval()

        with tqdm(loader, unit='sample', desc=f'Estimating uncertainties on {description}') as samples:
            for batch in samples:
                # Extract image and text inputs; ensure they are on the correct device
                xI, xT = batch[0].to(device), batch[1].to(device)

                with torch.no_grad():
                    # Extract CLIP features
                    xfI, xfT = CLIP_Net(xI, xT)

                # Compute image and text uncertainties
                (_, _, _, i_v), (_, _, _, t_v) = multi_fwpass_ProbVLM(
                    BayesCap_Net=Net,  
                    xfI=xfI,         
                    xfT=xfT,         
                    n_fw=n_fw,
                )

                print(f"Image uncert: {torch.mean(i_v)}")
                print(f"Text uncert:  {torch.mean(t_v)}")

                # Store uncertainties (mean across batch samples)
                image_uncertainties.append(torch.mean(i_v))
                text_uncertainties.append(torch.mean(t_v))

                # Free up GPU memory after each batch
                del xI, xT, xfI, xfT  # Delete batch variables to free memory
                torch.cuda.empty_cache()  # Clear unused memory

        # Average uncertainties over all batches
        avg_image_uncertainty = np.mean(image_uncertainties)
        avg_text_uncertainty = np.mean(text_uncertainties)
        return avg_image_uncertainty, avg_text_uncertainty

    print(f"Evaluating uncertainties on {iod_dataset} (in-distribution)")
    iod_image_uncertainty, iod_text_uncertainty = estimate_uncertainty_variances(iod_valid_loader, "IOD", Net, CLIP_Net, n_fw=n_fw)

    # Free up memory after IOD evaluation
    del iod_valid_loader
    torch.cuda.empty_cache()

    print(f"Evaluating uncertainties on {ood_dataset} (out-of-distribution)")
    ood_image_uncertainty, ood_text_uncertainty = estimate_uncertainty_variances(ood_valid_loader, "OOD", Net, CLIP_Net, n_fw=n_fw)

    # Free up memory after OOD evaluation
    del ood_valid_loader
    torch.cuda.empty_cache()

    # Print results
    print("\n*** Results ***")
    print(f"IOD: Avg. Image Uncertainty: {iod_image_uncertainty}, Avg. Text Uncertainty: {iod_text_uncertainty}")
    print(f"OOD: Avg. Image Uncertainty: {ood_image_uncertainty}, Avg. Text Uncertainty: {ood_text_uncertainty}")

    return {
        "IOD": {"image_uncertainty": iod_image_uncertainty, "text_uncertainty": iod_text_uncertainty},
        "OOD": {"image_uncertainty": ood_image_uncertainty, "text_uncertainty": ood_text_uncertainty},
    }


def main():
    model = "../ckpt/BBB_woKL_Net_best.pth"
    #eval_results = load_and_evaluate()
    #print(eval_results)
    #bin_recalls, bins = load_and_evaluate_uncert(ckpt_path=model)
    #print("Recall@1 for each bin:", bin_recalls)

    iod_ood = compare_iod_vs_ood(ckpt_path=model)


if __name__ == "__main__":
    main()

