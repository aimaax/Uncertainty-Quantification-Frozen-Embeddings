Week 45:
Thursday: 
    Read introductory paper (ProbVLM: Probabilistic Adapter for Frozen Vision-Language Models).
    Kick off meeting. 

Friday: 
    Set up of git and slack for communication. Started reading follow up paper (A review of uncertainty 
    quantification in deep learning: Techniques applications and challenges). Created report template. Started 
    creating presentation regarding summary of the introductory papers for meeting next Wednesday.

Week 46:
Monday: 
    Finished the presentation summarizing the papers. Started structuring what to mention in what sections in the report.

Tuesday: 
    Started writing introduction and theory in the report. 

Wednesday:	
    Meeting with Prashant where we presented the two papers mentioned above and discussed what we want to research more about.
    Need to decide if we e.g. want to do benchmark testing or/and see if we can both find the epistemic uncertainty from the deterministic 
    embeddings and also find the aleatoric uncertainty from the data and in which field we want to focus, continue on CLIP or try to find 
    uncertainties in e.g. healthcare. 

Thursday:
    Continue reading on related papers regarding choice of adapter. Looking into how to 
    separate aleatoric and epistemic uncertainties. Looking in to the Bayes By Backprop (BBB)
    adapter and how it could be implemented for the CLIP model. Looking into the ProbVLM code
    to understand how it works. Preparing questions and suggestions on how to move the project
    forward for a meeting with Prashant and Li tomorrow.

Friday:
    Meeting with Prashant and Li. Discussing how aleatoric and epistemic uncertainties work and 
    can be separated. Discussing the choice of adapter. Decided that we should start by implementing 
    the BBB BN. A second step would be to implement a separate term for the aleatoric uncertainty
    if the ProbVLM approach seems inadequate. Further down the line a benchmark against the ProbVLM
    approach would be implemented and visualizations with attention instead of diffusion.

    Preparing the Alvis environment and begin looking into how to implement the BBB. Looking into
    Bayesian Network layers using PyTorch.

Week 47:
Monday: 
    Preparing the alvis environment. Configuring VPN, and getting familiar with alvis.  
    Implementing a simple BNN as a proof of concept.

Tuesday:
    Downloading the COCO datasets to train the CLIP and ProbVLM model. Configuring the file structures and the datasets.
    Preparing a script that can be run as a batch job.
    Since downloading, extracting and moving the data to the cluster takes a long time we decided to write on the report
    and try submitting the batch job tomrorrow, so the data can load over the night.

Wednesday:
    Managed to get the whole ProbVLM pipeline to work on alvis (alot of debugging). 
    Tried training and evaluating a simple BNN which is working. 
    Downloaded all of the other datasets to evaluate uncertainties iid and ood, i.e. train on one of the dataset e.g. COCO and test it on the others.  

Thursday:
    Meeting with Prashant and collegues. Discussed our implementation, choice of loss function.
    Discussing next steps. The next steps are to train the entire ProbVLM and our BBB model on alvis.
    We are first gonna implement early stopping, adaptive learning rate, change the priors according
    to documentation. Then we are gonna train both models and time them (that is important so we can
    compare the computational complexities of both models, ours should be much faster than ProbVLM
    since they are utilizing drop-out which is a quite expensive operation). 

    We also discussed how we should evaluate our model. One thing to just sanity check is to check if
    out of distribution samples have higher uncertainties than in distribution samples. We could 
    also produce the roc recall-uncertainty curves to compare with probvlm. For the real evaluation
    we discussed using a downstream task such as captioning, or using guided attention, so we can 
    compare the methods on something real.


Friday:


Week 48:
    To do list: 
    	1. Investigate sigma prior and mu prior
	2. Investigate adaptive learning rate (problems with MSE jumping)
	3. Rerun uncertainty estimate for when the new BBB algo is done
	4. PCA to visualize the embedding space
	5. Produce ROC-curves
	6. Downstream task

    Questions for Prashant on Friday:
	1. Mention Max's update to the BNN architecture, does it make sense?
	2. Discuss adaptive learning rate and early stopping (if we want to compare time)
		also ProbVLM doesn't use adaptive learning rate and they have fixed epochs
		(though they have an adaptive learner but it steps outside the for loop????)

		ProbVLM was faster than BBB per epoch
		Also evaluation MSE and MAE was better for ProbVLM but better loss for BBB.
		Maybe this is not relevant since we only care about estimating unceratinty.

	3. Present training plots. Does everything looks good. Very low MSE and MAE. 
		Loss for BBB is higher (but also a second term). Does KL look right?

	3. I believe they are estimating the uncertainty wrong. They are embedding the aleatoric uncertainty into the epistemic uncertainty
	   	And their aleatoric uncertainty looks weird, and computes extremely big values.
	   	We emailed ProbVLM and he tried to explain to us how they recreated the roc-curves, but they didn't have any code.
	   	We are thinking about rewriting the uncertainty code but then its hard to recreate the plots from 
		the Paper and know that we are correct.

		Also why are they clipping the values in the get_GGuncer code?

	4. In the mail he recomended using a zero-shot classification setting and using standard calibation metrics to estimate uncertainty.
		Is this something that is relevant for us. Calibartion is another technique or not?

	5. Downstream task discussion

Monday:
	Training ProbVLM and BBB (with the additional KL term in the loss).
	Setting up the flickr dataset, and evaluating it. Writing a script to extract the epistemic uncertainty in order to compare the uncertainty
	for coco vs flickr (coco should be lower since it is trained on it).
	Trying to debug the recall script (something is still wrong).
 
Tuesday:
	Debugging what is going wrong with the models and evaluation script. Checking the feature vectors of the samples using PCA. The trained BBB
	seemed to collapse in the center while the ProbVLM looked better after the dimensionality reduction using PCA. So something went wrong with the 
	BBB training which we are also trying to debug. It may be the prior of mu and sigma or that we did not include the KL divergence term at first. 
	Retraining the model is ongoing.     

Wednesday:
	Retrain the models. Change the BNN complexity and structure, ProbVLM correct number of hidden dimensions. Try to debug the uncertainty code.
	Reach out to the ProbVLM authors for help on how to recreate the ROC-curve.

Thursday:
	Figured out what is wrong with the uncertainty. The aleatoric get_ggUncer functino is very weird. 
	Leading to very big values. But wihtout it epistemic uncertainty is very small.
	ProbVLM authors answered and gave us some tips, but didn't have their own implementation saved for the evaluation.
	
Friday:
	Meeting with the group. BBB architecture was okay except that we need to add the expaction value to the loss the get the noise of the model.
	Otherwise the model continues to be deterministic. Training looked OK for MSE, MAE and KL term. ProbVLM paper is wrong about the calculation of 
	uncertainty. Fix downstream task to be able to test the model, because now we are looking a bit blind to the metrics and we actually don't know
	how it performs. Zero-shot classification, image-captioning should be tried as downstream. 


Potential reasons for the uncertainty problems:
	1. The expectation term
		altough i already think we do that when we sample the parameters from the network in the for loop in main.py
	2. Not enough stocasisity in the network
		Solution: add more bayesian layers
	3. Check this out: https://github.com/ykwon0407/UQ_BNN/blob/master/retina/utils.py

Week 49:

Monday:
	First downstream task implemented for zero-shot classification using ImageNet. Lecture. Debug ROC as usual. 

Tuesday: 
	Implemented gradient for alpha and beta. Cifar100 for zero-shot classification. Tried to install polos on alvis but polos are using a too old
	python which doesn't make it possible on alvis. We now know which recall to use and that the ProbVLM paper doesn't tell the same in the paper
	as they do in the code so we have to make up our own conclusions in the end which makes it not totally comparable. Also having contact with 
	the ProbVLM authors. 

Wednesday: 	
	Been getting out the recall - uncertainty plot but it is reversed to what we want it to be. Retraining more models and changing 
	the weight and reduction of the KL term for BBB. Also experimenting what the T1 and T2 affect the models. Preparing presentation 
	for Monday. 

Thursday: 
	Trying to figure out what is wrong with our alpha term in the GGD, it becomes very small and we have tried to retrain the models with 
	optimization to make alpha larger. Something is wrong and the main authors for ProbVLM "don't have access" to last checkpoint and the code
	is as mentioned, not reproducable, in which we are thinking that the code and result of the main paper is kind of cheating. So we are trying
	to reformulate the loss function to a standard Gaussian with loss of intra-modal and cross-modal term + KL divergence for Bayesian Linear Layers
	, this to get rid of the instability issues from the loss derived from the GGD pdf. This was discussed with Prashant and Liu in todays meeting. 
	
Friday:
	Retraining the ProbVLM from the main paper to prove that the model does not work as intended. Also mathematically derived the new loss function
	and implementing it to the code to soon retrain the model with our own approach. Hopefully it works better :) 

