Week 45:
Thursday: 
    Read introductory paper (ProbVLM: Probabilistic Adapter for Frozen Vision-Language Models).
    Kick off meeting. 

Friday: 
    Set up of git and slack for communication. Started reading follow up paper (A review of uncertainty 
    quantification in deep learning: Techniques applications and challenges). Created report template. Started 
    creating presentation regarding summary of the introductory papers for meeting next Wednesday.

Week 46:
Monday: 
    Finished the presentation summarizing the papers. Started structuring what to mention in what sections in the report.

Tuesday: 
    Started writing introduction and theory in the report. 

Wednesday:	
    Meeting with Prashant where we presented the two papers mentioned above and discussed what we want to research more about.
    Need to decide if we e.g. want to do benchmark testing or/and see if we can both find the epistemic uncertainty from the deterministic 
    embeddings and also find the aleatoric uncertainty from the data and in which field we want to focus, continue on CLIP or try to find 
    uncertainties in e.g. healthcare. 

Thursday:
    Continue reading on related papers regarding choice of adapter. Looking into how to 
    separate aleatoric and epistemic uncertainties. Looking in to the Bayes By Backprop (BBB)
    adapter and how it could be implemented for the CLIP model. Looking into the ProbVLM code
    to understand how it works. Preparing questions and suggestions on how to move the project
    forward for a meeting with Prashant and Li tomorrow.

Friday:
    Meeting with Prashant and Li. Discussing how aleatoric and epistemic uncertainties work and 
    can be separated. Discussing the choice of adapter. Decided that we should start by implementing 
    the BBB BN. A second step would be to implement a separate term for the aleatoric uncertainty
    if the ProbVLM approach seems inadequate. Further down the line a benchmark against the ProbVLM
    approach would be implemented and visualizations with attention instead of diffusion.

    Preparing the Alvis environment and begin looking into how to implement the BBB. Looking into
    Bayesian Network layers using PyTorch.

Week 47:
Monday: 
    Preparing the alvis environment. Configuring VPN, and getting familiar with alvis.  
    Implementing a simple BNN as a proof of concept.

Tuesday:
    Downloading the COCO datasets to train the CLIP and ProbVLM model. Configuring the file structures and the datasets.
    Preparing a script that can be run as a batch job.
    Since downloading, extracting and moving the data to the cluster takes a long time we decided to write on the report
    and try submitting the batch job tomrorrow, so the data can load over the night.

Wednesday:
    Managed to get the whole ProbVLM pipeline to work on alvis (alot of debugging). 
    Tried training and evaluating a simple BNN which is working. 
    Downloaded all of the other datasets to evaluate uncertainties iid and ood, i.e. train on one of the dataset e.g. COCO and test it on the others.  

Thursday:
    Meeting with Prashant and collegues. Discussed our implementation, choice of loss function.
    Discussing next steps. The next steps are to train the entire ProbVLM and our BBB model on alvis.
    We are first gonna implement early stopping, adaptive learning rate, change the priors according
    to documentation. Then we are gonna train both models and time them (that is important so we can
    compare the computational complexities of both models, ours should be much faster than ProbVLM
    since they are utilizing drop-out which is a quite expensive operation). 

    We also discussed how we should evaluate our model. One thing to just sanity check is to check if
    out of distribution samples have higher uncertainties than in distribution samples. We could 
    also produce the roc recall-uncertainty curves to compare with probvlm. For the real evaluation
    we discussed using a downstream task such as captioning, or using guided attention, so we can 
    compare the methods on something real.


Friday:


Week 48:
    To do list. 
    Train ProbVLM for reference
    Add KL divergence term to loss function
    Evaluate uncertainty for out of distribution data

Monday:
	Training ProbVLM and BBB (with the additional KL term in the loss).
	Setting up the flickr dataset, and evaluating it. Writing a script to extract the epistemic uncertainty in order to compare the uncertainty
	for coco vs flickr (coco should be lower since it is trained on it).
	Trying to debug the recall script (something is still wrong).
    
